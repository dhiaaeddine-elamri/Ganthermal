import torch
import torch.nn as nn
from model import generator
from denoising_module import denoiser
from patchdiscriminator import PatchGan_discriminator

def check_gpu_usage():
    print("=== V√©rification GPU ===")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU count: {torch.cuda.device_count()}")
        print(f"Current device: {torch.cuda.current_device()}")
        print(f"Device name: {torch.cuda.get_device_name(0)}")
        print(f"CUDA version: {torch.version.cuda}")
        
        # Test GPU memory
        torch.cuda.empty_cache()
        print(f"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
        print(f"GPU memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB")
    else:
        print("‚ùå CUDA non disponible!")
        return False
    
    print("\n=== Test des mod√®les sur GPU ===")
    
    # Cr√©er un tenseur de test
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    test_tensor = torch.randn(2, 1, 64, 64).to(device)
    print(f"Test tensor device: {test_tensor.device}")
    
    # Tester les mod√®les
    try:
        # G√©n√©rateur
        gen_model = generator().to(device)
        gen_output = gen_model(test_tensor)
        print(f"‚úÖ Generator: {gen_output.device}, shape: {gen_output.shape}")
        
        # D√©bruiteur
        denoiser_model = denoiser().to(device)
        denoiser_output = denoiser_model(test_tensor)
        print(f"‚úÖ Denoiser: {denoiser_output.device}, shape: {denoiser_output.shape}")
        
        # Discriminateur
        disc_model = PatchGan_discriminator().to(device)
        disc_output = disc_model(test_tensor)
        print(f"‚úÖ Discriminator: {disc_output.device}, shape: {disc_output.shape}")
        
        # V√©rifier la m√©moire GPU apr√®s chargement des mod√®les
        print(f"\nGPU memory after loading models: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
        
        # Test de performance simple
        print("\n=== Test de performance ===")
        import time
        
        start_time = time.time()
        for _ in range(10):
            _ = gen_model(test_tensor)
        end_time = time.time()
        
        print(f"Temps pour 10 forward passes: {end_time - start_time:.4f} secondes")
        
        # V√©rifier que les param√®tres sont sur GPU
        print("\n=== V√©rification des param√®tres ===")
        for name, param in gen_model.named_parameters():
            if param.device.type != 'cuda':
                print(f"‚ùå {name}: {param.device}")
            else:
                print(f"‚úÖ {name}: {param.device}")
                break  # Juste le premier pour √©viter trop de sortie
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {e}")
        return False

if __name__ == "__main__":
    success = check_gpu_usage()
    if success:
        print("\nüéâ GPU fonctionne correctement!")
    else:
        print("\nüí• Probl√®me avec le GPU!") 